{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import LongTensor\n",
    "from torch.nn import Embedding, LSTM\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We want to run LSTM on a batch of 3 character sequences ['long_str', 'tiny', 'medium']\n",
    "#\n",
    "#     Step 1: Construct Vocabulary\n",
    "#     Step 2: Load indexed data (list of instances, where each instance is list of character indices)\n",
    "#     Step 3: Make Model\n",
    "#  *  Step 4: Pad instances with 0s till max length sequence\n",
    "#  *  Step 5: Sort instances by sequence length in descending order\n",
    "#  *  Step 6: Embed the instances\n",
    "#  *  Step 7: Call pack_padded_sequence with embeded instances and sequence lengths\n",
    "#  *  Step 8: Forward with LSTM\n",
    "#  *  Step 9: Call unpack_padded_sequences if required / or just pick last hidden vector\n",
    "#  *  Summary of Shape Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to run LSTM on a batch following 3 character sequences\n",
    "seqs = ['long_str',  # len = 8\n",
    "        'tiny',      # len = 4\n",
    "        'medium']    # len = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Construct Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', '_', 'd', 'e', 'g', 'i', 'l', 'm', 'n', 'o', 'r', 's', 't', 'u', 'y']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##------------------------------##\n",
    "# make sure <pad> idx is 0\n",
    "vocab = ['<pad>'] + sorted(set([char for seq in seqs for char in seq]))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Load indexed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6, 9, 8, 4, 1, 11, 12, 10], [12, 5, 8, 14], [7, 3, 2, 5, 13, 7]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of instances, where each instance is list of character indices\n",
    "# vectorized_seqs => [[6, 9, 8, 4, 1, 11, 12, 10],   --> 'long_str'\n",
    "#                     [12, 5, 8, 14],                --> 'tiny'\n",
    "#                     [7, 3, 2, 5, 13, 7]]           --> 'medium'\n",
    "\n",
    "vectorized_seqs = [[vocab.index(tok) for tok in seq]for seq in seqs]\n",
    "vectorized_seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Make Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(15, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build work embedding for encoding\n",
    "embed = Embedding(len(vocab), 4) # embedding_dim = 4\n",
    "embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(4, 5, batch_first=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a LSTM model\n",
    "lstm = LSTM(input_size=4, hidden_size=5, batch_first=True) # input_dim = 4, hidden_dim = 5\n",
    "lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Pad instances with 0s till max length sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 4, 6])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the length of each seq in your batch\n",
    "# seq_lengths => [ 8, 4,  6]\n",
    "# batch_sum_seq_len: 8 + 4 + 6 = 18\n",
    "# max_seq_len: 8\n",
    "seq_lengths = LongTensor(list(map(len, vectorized_seqs)))\n",
    "seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_tensor = Variable(torch.zeros((len(vectorized_seqs), seq_lengths.max()))).long()\n",
    "seq_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6,  9,  8,  4,  1, 11, 12, 10],\n",
       "        [12,  5,  8, 14,  0,  0,  0,  0],\n",
       "        [ 7,  3,  2,  5, 13,  7,  0,  0]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "    seq_tensor[idx, :seqlen] = LongTensor(seq)\n",
    "\n",
    "# seq_tensor.shape : (batch_size X max_seq_len) = (3 X 8)\n",
    "seq_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Sort instances by sequence length in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6,  9,  8,  4,  1, 11, 12, 10],\n",
       "        [ 7,  3,  2,  5, 13,  7,  0,  0],\n",
       "        [12,  5,  8, 14,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
    "seq_tensor = seq_tensor[perm_idx]\n",
    "\n",
    "# seq_tensor => [[ 6  9  8  4  1 11 12 10]           # long_str\n",
    "#                [ 7  3  2  5 13  7  0  0]           # medium\n",
    "#                [12  5  8 14  0  0  0  0]]          # tiny\n",
    "# seq_tensor.shape : (batch_size X max_seq_len) = (3 X 8)\n",
    "seq_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_lengths: tensor([8, 4, 6]) -->  tensor([8, 6, 4])\n",
      "perm_idx tensor([0, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "print(\"seq_lengths: tensor([8, 4, 6]) --> \", seq_lengths)\n",
    "print(\"perm_idx\", perm_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Embed the instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-6.6082e-01, -1.0030e+00, -2.1691e+00, -9.9223e-01],\n",
       "         [ 1.1729e+00,  3.9620e-02,  6.6538e-02,  8.4358e-01],\n",
       "         [-4.6659e-01, -3.0592e-01,  8.5859e-01,  1.7894e+00],\n",
       "         [ 5.9985e-01, -7.2729e-01,  1.6543e+00, -4.0921e-03],\n",
       "         [ 4.4786e-01,  1.8161e+00, -2.0536e+00, -1.8928e+00],\n",
       "         [ 4.6203e-02, -8.1840e-02,  1.1860e-01, -1.1929e+00],\n",
       "         [ 1.0607e+00, -1.7305e-03,  1.7065e-01,  2.6640e-01],\n",
       "         [ 6.4725e-01,  1.0117e+00, -5.0601e-02,  1.0182e+00]],\n",
       "\n",
       "        [[ 1.0390e+00,  4.2287e-02,  8.3849e-01,  1.8263e-01],\n",
       "         [-1.4074e+00, -8.1444e-01, -2.3716e-01,  2.3193e-01],\n",
       "         [-6.2568e-01,  1.4961e+00, -1.0552e+00, -6.3288e-01],\n",
       "         [-6.9315e-01, -7.1027e-01,  1.2622e+00, -5.8106e-01],\n",
       "         [-1.5714e+00,  5.8442e-01, -1.2303e+00, -5.0362e-01],\n",
       "         [ 1.0390e+00,  4.2287e-02,  8.3849e-01,  1.8263e-01],\n",
       "         [ 5.6696e-02,  6.3566e-01, -7.6484e-01,  2.8574e-01],\n",
       "         [ 5.6696e-02,  6.3566e-01, -7.6484e-01,  2.8574e-01]],\n",
       "\n",
       "        [[ 1.0607e+00, -1.7305e-03,  1.7065e-01,  2.6640e-01],\n",
       "         [-6.9315e-01, -7.1027e-01,  1.2622e+00, -5.8106e-01],\n",
       "         [-4.6659e-01, -3.0592e-01,  8.5859e-01,  1.7894e+00],\n",
       "         [ 5.2095e-01, -1.2459e+00,  1.3618e-01,  2.8436e-02],\n",
       "         [ 5.6696e-02,  6.3566e-01, -7.6484e-01,  2.8574e-01],\n",
       "         [ 5.6696e-02,  6.3566e-01, -7.6484e-01,  2.8574e-01],\n",
       "         [ 5.6696e-02,  6.3566e-01, -7.6484e-01,  2.8574e-01],\n",
       "         [ 5.6696e-02,  6.3566e-01, -7.6484e-01,  2.8574e-01]]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_seq_tensor = embed(seq_tensor)\n",
    "# tensor([[[-6.6082e-01, -1.0030e+00, -2.1691e+00, -9.9223e-01],   --> l\n",
    "#          [ 1.1729e+00,  3.9620e-02,  6.6538e-02,  8.4358e-01],   --> o\n",
    "#          [-4.6659e-01, -3.0592e-01,  8.5859e-01,  1.7894e+00],   --> n\n",
    "#          [ 5.9985e-01, -7.2729e-01,  1.6543e+00, -4.0921e-03],   --> g\n",
    "#          [ 4.4786e-01,  1.8161e+00, -2.0536e+00, -1.8928e+00],   --> _\n",
    "#          [ 4.6203e-02, -8.1840e-02,  1.1860e-01, -1.1929e+00],   --> s\n",
    "#          [ 1.0607e+00, -1.7305e-03,  1.7065e-01,  2.6640e-01],   --> t\n",
    "#          [ 6.4725e-01,  1.0117e+00, -5.0601e-02,  1.0182e+00]],  --> r\n",
    "\n",
    "#         [[ 1.0390e+00,  4.2287e-02,  8.3849e-01,  1.8263e-01],   --> m\n",
    "#          [-1.4074e+00, -8.1444e-01, -2.3716e-01,  2.3193e-01],   --> e\n",
    "#          [-6.2568e-01,  1.4961e+00, -1.0552e+00, -6.3288e-01],   --> d\n",
    "#          [-6.9315e-01, -7.1027e-01,  1.2622e+00, -5.8106e-01],   --> i\n",
    "#          [-1.5714e+00,  5.8442e-01, -1.2303e+00, -5.0362e-01],   --> u\n",
    "#          [ 1.0390e+00,  4.2287e-02,  8.3849e-01,  1.8263e-01],   --> m \n",
    "#          [ 5.6696e-02,  6.3566e-01, -7.6484e-01,  2.8574e-01],   --> <pad>\n",
    "#          [ 5.6696e-02,  6.3566e-01, -7.6484e-01,  2.8574e-01]],  --> <pad>\n",
    "\n",
    "#         [[ 1.0607e+00, -1.7305e-03,  1.7065e-01,  2.6640e-01],   --> t\n",
    "#          [-6.9315e-01, -7.1027e-01,  1.2622e+00, -5.8106e-01],   --> i\n",
    "#          [-4.6659e-01, -3.0592e-01,  8.5859e-01,  1.7894e+00],   --> n\n",
    "#          [ 5.2095e-01, -1.2459e+00,  1.3618e-01,  2.8436e-02],   --> y\n",
    "#          [ 5.6696e-02,  6.3566e-01, -7.6484e-01,  2.8574e-01],   --> <pad>\n",
    "#          [ 5.6696e-02,  6.3566e-01, -7.6484e-01,  2.8574e-01],   --> <pad>\n",
    "#          [ 5.6696e-02,  6.3566e-01, -7.6484e-01,  2.8574e-01],   --> <pad>\n",
    "#          [ 5.6696e-02,  6.3566e-01, -7.6484e-01,  2.8574e-01]]], --> <pad>\n",
    "#        grad_fn=<EmbeddingBackward>)\n",
    "# embedded_seq_tensor.shape : (batch_size X max_seq_len X embedding_dim) = (3 X 8 X 4)\n",
    "embedded_seq_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Padded embedded instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[-6.6082e-01, -1.0030e+00, -2.1691e+00, -9.9223e-01],\n",
       "        [ 1.0390e+00,  4.2287e-02,  8.3849e-01,  1.8263e-01],\n",
       "        [ 1.0607e+00, -1.7305e-03,  1.7065e-01,  2.6640e-01],\n",
       "        [ 1.1729e+00,  3.9620e-02,  6.6538e-02,  8.4358e-01],\n",
       "        [-1.4074e+00, -8.1444e-01, -2.3716e-01,  2.3193e-01],\n",
       "        [-6.9315e-01, -7.1027e-01,  1.2622e+00, -5.8106e-01],\n",
       "        [-4.6659e-01, -3.0592e-01,  8.5859e-01,  1.7894e+00],\n",
       "        [-6.2568e-01,  1.4961e+00, -1.0552e+00, -6.3288e-01],\n",
       "        [-4.6659e-01, -3.0592e-01,  8.5859e-01,  1.7894e+00],\n",
       "        [ 5.9985e-01, -7.2729e-01,  1.6543e+00, -4.0921e-03],\n",
       "        [-6.9315e-01, -7.1027e-01,  1.2622e+00, -5.8106e-01],\n",
       "        [ 5.2095e-01, -1.2459e+00,  1.3618e-01,  2.8436e-02],\n",
       "        [ 4.4786e-01,  1.8161e+00, -2.0536e+00, -1.8928e+00],\n",
       "        [-1.5714e+00,  5.8442e-01, -1.2303e+00, -5.0362e-01],\n",
       "        [ 4.6203e-02, -8.1840e-02,  1.1860e-01, -1.1929e+00],\n",
       "        [ 1.0390e+00,  4.2287e-02,  8.3849e-01,  1.8263e-01],\n",
       "        [ 1.0607e+00, -1.7305e-03,  1.7065e-01,  2.6640e-01],\n",
       "        [ 6.4725e-01,  1.0117e+00, -5.0601e-02,  1.0182e+00]],\n",
       "       grad_fn=<PackPaddedSequenceBackward>), batch_sizes=tensor([3, 3, 3, 3, 2, 2, 1, 1]), sorted_indices=None, unsorted_indices=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call pack_padded_sequence with embeded instances and sequence lengths\n",
    "# packed_input (PackedSequence is NamedTuple with 2 attributes: data and batch_sizes\n",
    "packed_input = pack_padded_sequence(embedded_seq_tensor, seq_lengths.cpu().numpy(), batch_first=True)\n",
    "packed_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PackedSequence(data=tensor([\n",
    "#         [-6.6082e-01, -1.0030e+00, -2.1691e+00, -9.9223e-01],   --> l\n",
    "#         [ 1.0390e+00,  4.2287e-02,  8.3849e-01,  1.8263e-01],   --> m\n",
    "#         [ 1.0607e+00, -1.7305e-03,  1.7065e-01,  2.6640e-01],   --> t\n",
    "\n",
    "#         [ 1.1729e+00,  3.9620e-02,  6.6538e-02,  8.4358e-01],   --> o\n",
    "#         [-1.4074e+00, -8.1444e-01, -2.3716e-01,  2.3193e-01],   --> e\n",
    "#         [-6.9315e-01, -7.1027e-01,  1.2622e+00, -5.8106e-01],   --> i\n",
    "\n",
    "#         [-4.6659e-01, -3.0592e-01,  8.5859e-01,  1.7894e+00],   --> n\n",
    "#         [-6.2568e-01,  1.4961e+00, -1.0552e+00, -6.3288e-01],   --> d\n",
    "#         [-4.6659e-01, -3.0592e-01,  8.5859e-01,  1.7894e+00],   --> n\n",
    "\n",
    "#         [ 5.9985e-01, -7.2729e-01,  1.6543e+00, -4.0921e-03],   --> g\n",
    "#         [-6.9315e-01, -7.1027e-01,  1.2622e+00, -5.8106e-01],   --> i\n",
    "#         [ 5.2095e-01, -1.2459e+00,  1.3618e-01,  2.8436e-02],   --> y\n",
    "\n",
    "#         [ 4.4786e-01,  1.8161e+00, -2.0536e+00, -1.8928e+00],   --> _\n",
    "#         [-1.5714e+00,  5.8442e-01, -1.2303e+00, -5.0362e-01],   --> u\n",
    "\n",
    "#         [ 4.6203e-02, -8.1840e-02,  1.1860e-01, -1.1929e+00],   --> s\n",
    "#         [ 1.0390e+00,  4.2287e-02,  8.3849e-01,  1.8263e-01],   --> m \n",
    "\n",
    "#         [ 1.0607e+00, -1.7305e-03,  1.7065e-01,  2.6640e-01],   --> t\n",
    "\n",
    "#         [ 6.4725e-01,  1.0117e+00, -5.0601e-02,  1.0182e+00]    --> r\n",
    "# ],grad_fn=<PackPaddedSequenceBackward>), \n",
    "# batch_sizes=tensor([3, 3, 3, 3, 2, 2, 1, 1]), sorted_indices=None, unsorted_indices=None)\n",
    "\n",
    "# visualization :\n",
    "# l  o  n  g  _  s  t  r   #(long_str)\n",
    "# m  e  d  i  u  m         #(medium)\n",
    "# t  i  n  y               #(tiny)\n",
    "# 3  3  3  3  2  2  1  1   (sum = 18 [batch_sum_seq_len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Forward with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[ 0.0305, -0.0686, -0.1213,  0.2604,  0.1534],\n",
       "        [-0.1996,  0.1842,  0.1184, -0.0071,  0.0386],\n",
       "        [-0.1373,  0.1392,  0.0782,  0.0196,  0.0906],\n",
       "        [-0.0504,  0.0870, -0.0280,  0.1770,  0.1564],\n",
       "        [-0.0504,  0.0071,  0.0529,  0.2341,  0.0088],\n",
       "        [-0.2570,  0.2009,  0.2084,  0.1811, -0.0631],\n",
       "        [-0.0495,  0.1820,  0.1432,  0.1139,  0.0452],\n",
       "        [-0.2350, -0.0707, -0.1286,  0.1738,  0.1241],\n",
       "        [-0.1722,  0.2153,  0.2802,  0.1195, -0.0604],\n",
       "        [-0.2376,  0.3563,  0.2385,  0.0958, -0.0380],\n",
       "        [-0.2768,  0.1355,  0.0127,  0.2670, -0.0711],\n",
       "        [-0.1071,  0.2318,  0.2240,  0.2047,  0.0324],\n",
       "        [-0.2878, -0.0577, -0.0485,  0.1638,  0.1690],\n",
       "        [-0.2172, -0.0717, -0.1759,  0.2423,  0.0177],\n",
       "        [-0.3456,  0.0538, -0.0787,  0.2602,  0.1533],\n",
       "        [-0.3336,  0.1145, -0.0381,  0.1986,  0.0484],\n",
       "        [-0.3414,  0.1961,  0.0077,  0.2441,  0.1773],\n",
       "        [-0.3967,  0.1628,  0.0038,  0.1719,  0.2116]], grad_fn=<CatBackward>), batch_sizes=tensor([3, 3, 3, 3, 2, 2, 1, 1]), sorted_indices=None, unsorted_indices=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# packed_output (PackedSequence is NamedTuple with 2 attributes: data and batch_sizes\n",
    "packed_output, (ht, ct) = lstm(packed_input)\n",
    "packed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packed_output.data.shape : (batch_sum_seq_len X hidden_dim) = (18 X 5)\n",
    "# PackedSequence(data=tensor([\n",
    "#         [ 0.0305, -0.0686, -0.1213,  0.2604,  0.1534],   --> l\n",
    "#         [-0.1996,  0.1842,  0.1184, -0.0071,  0.0386],   --> m\n",
    "#         [-0.1373,  0.1392,  0.0782,  0.0196,  0.0906],   --> t\n",
    "\n",
    "#         [-0.0504,  0.0870, -0.0280,  0.1770,  0.1564],   --> o\n",
    "#         [-0.0504,  0.0071,  0.0529,  0.2341,  0.0088],   --> e\n",
    "#         [-0.2570,  0.2009,  0.2084,  0.1811, -0.0631],   --> i\n",
    "\n",
    "#         [-0.0495,  0.1820,  0.1432,  0.1139,  0.0452],   --> n\n",
    "#         [-0.2350, -0.0707, -0.1286,  0.1738,  0.1241],   --> d\n",
    "#         [-0.1722,  0.2153,  0.2802,  0.1195, -0.0604],   --> n\n",
    "\n",
    "#         [-0.2376,  0.3563,  0.2385,  0.0958, -0.0380],   --> g\n",
    "#         [-0.2768,  0.1355,  0.0127,  0.2670, -0.0711],   --> i\n",
    "#         [-0.1071,  0.2318,  0.2240,  0.2047,  0.0324],   --> y\n",
    "\n",
    "#         [-0.2878, -0.0577, -0.0485,  0.1638,  0.1690],   --> _\n",
    "#         [-0.2172, -0.0717, -0.1759,  0.2423,  0.0177],   --> u\n",
    "\n",
    "#         [-0.3456,  0.0538, -0.0787,  0.2602,  0.1533],   --> s\n",
    "#         [-0.3336,  0.1145, -0.0381,  0.1986,  0.0484],   --> m \n",
    "\n",
    "#         [-0.3414,  0.1961,  0.0077,  0.2441,  0.1773],   --> t\n",
    "\n",
    "#         [-0.3967,  0.1628,  0.0038,  0.1719,  0.2116]   --> r\n",
    "# ], grad_fn=<CatBackward>), \n",
    "\n",
    "# batch_sizes=tensor([3, 3, 3, 3, 2, 2, 1, 1]), sorted_indices=None, unsorted_indices=None)\n",
    "# visualization :\n",
    "# l  o  n  g  _  s  t  r   #(long_str)\n",
    "# m  e  d  i  u  m         #(medium)\n",
    "# t  i  n  y               #(tiny)\n",
    "# 3  3  3  3  2  2  1  1   (sum = 18 [batch_sum_seq_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3967,  0.1628,  0.0038,  0.1719,  0.2116],\n",
       "         [-0.3336,  0.1145, -0.0381,  0.1986,  0.0484],\n",
       "         [-0.1071,  0.2318,  0.2240,  0.2047,  0.0324]]],\n",
       "       grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The hidden output of the last layer of each sub_lstm model\n",
    "# tensor([[[-0.3967,  0.1628,  0.0038,  0.1719,  0.2116],    --> r\n",
    "#          [-0.3336,  0.1145, -0.0381,  0.1986,  0.0484],    --> m \n",
    "#          [-0.1071,  0.2318,  0.2240,  0.2047,  0.0324]]],  --> y\n",
    "#        grad_fn=<StackBackward>)\n",
    "ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8016,  0.4717,  0.0093,  0.3356,  0.4591],\n",
       "         [-0.6821,  0.2103, -0.0774,  0.3886,  0.1060],\n",
       "         [-0.3206,  0.5972,  0.5197,  0.3295,  0.0666]]],\n",
       "       grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor([[[-0.8016,  0.4717,  0.0093,  0.3356,  0.4591],\n",
    "#          [-0.6821,  0.2103, -0.0774,  0.3886,  0.1060],\n",
    "#          [-0.3206,  0.5972,  0.5197,  0.3295,  0.0666]]],\n",
    "#        grad_fn=<StackBackward>)\n",
    "# ct.shape, torch.Size([1, 3, 5])\n",
    "ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: unpack padded output sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call unpack_padded_sequences if required / or just pick last hidden vector\n",
    "# unpack your output if required\n",
    "output, input_sizes = pad_packed_sequence(packed_output, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0305, -0.0686, -0.1213,  0.2604,  0.1534],\n",
       "         [-0.0504,  0.0870, -0.0280,  0.1770,  0.1564],\n",
       "         [-0.0495,  0.1820,  0.1432,  0.1139,  0.0452],\n",
       "         [-0.2376,  0.3563,  0.2385,  0.0958, -0.0380],\n",
       "         [-0.2878, -0.0577, -0.0485,  0.1638,  0.1690],\n",
       "         [-0.3456,  0.0538, -0.0787,  0.2602,  0.1533],\n",
       "         [-0.3414,  0.1961,  0.0077,  0.2441,  0.1773],\n",
       "         [-0.3967,  0.1628,  0.0038,  0.1719,  0.2116]],\n",
       "\n",
       "        [[-0.1996,  0.1842,  0.1184, -0.0071,  0.0386],\n",
       "         [-0.0504,  0.0071,  0.0529,  0.2341,  0.0088],\n",
       "         [-0.2350, -0.0707, -0.1286,  0.1738,  0.1241],\n",
       "         [-0.2768,  0.1355,  0.0127,  0.2670, -0.0711],\n",
       "         [-0.2172, -0.0717, -0.1759,  0.2423,  0.0177],\n",
       "         [-0.3336,  0.1145, -0.0381,  0.1986,  0.0484],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.1373,  0.1392,  0.0782,  0.0196,  0.0906],\n",
       "         [-0.2570,  0.2009,  0.2084,  0.1811, -0.0631],\n",
       "         [-0.1722,  0.2153,  0.2802,  0.1195, -0.0604],\n",
       "         [-0.1071,  0.2318,  0.2240,  0.2047,  0.0324],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor([[[ 0.0305, -0.0686, -0.1213,  0.2604,  0.1534],   --> l\n",
    "#          [-0.0504,  0.0870, -0.0280,  0.1770,  0.1564],   --> o\n",
    "#          [-0.0495,  0.1820,  0.1432,  0.1139,  0.0452],   --> n\n",
    "#          [-0.2376,  0.3563,  0.2385,  0.0958, -0.0380],   --> g\n",
    "#          [-0.2878, -0.0577, -0.0485,  0.1638,  0.1690],   --> _\n",
    "#          [-0.3456,  0.0538, -0.0787,  0.2602,  0.1533],   --> s\n",
    "#          [-0.3414,  0.1961,  0.0077,  0.2441,  0.1773],   --> t\n",
    "#          [-0.3967,  0.1628,  0.0038,  0.1719,  0.2116]],  --> r\n",
    "\n",
    "#         [[-0.1996,  0.1842,  0.1184, -0.0071,  0.0386],   --> m\n",
    "#          [-0.0504,  0.0071,  0.0529,  0.2341,  0.0088],   --> e\n",
    "#          [-0.2350, -0.0707, -0.1286,  0.1738,  0.1241],   --> d\n",
    "#          [-0.2768,  0.1355,  0.0127,  0.2670, -0.0711],   --> i\n",
    "#          [-0.2172, -0.0717, -0.1759,  0.2423,  0.0177],   --> u\n",
    "#          [-0.3336,  0.1145, -0.0381,  0.1986,  0.0484],   --> m \n",
    "#          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],   --> <pad>\n",
    "#          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],  --> <pad>\n",
    "\n",
    "#         [[-0.1373,  0.1392,  0.0782,  0.0196,  0.0906],   --> t\n",
    "#          [-0.2570,  0.2009,  0.2084,  0.1811, -0.0631],   --> i\n",
    "#          [-0.1722,  0.2153,  0.2802,  0.1195, -0.0604],   --> n\n",
    "#          [-0.1071,  0.2318,  0.2240,  0.2047,  0.0324],   --> y\n",
    "#          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],   --> <pad>\n",
    "#          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],   --> <pad>\n",
    "#          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],   --> <pad>\n",
    "#          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]   --> <pad>\n",
    "#        ],\n",
    "#        grad_fn=<TransposeBackward0>)\n",
    "\n",
    "# output.shape : ( batch_size X max_seq_len X hidden_dim) = (3 X 8 X 5)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 6, 4])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Summary of Shape Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# (batch_size X max_seq_len X embedding_dim) --> Sort by seqlen ---> (batch_size X max_seq_len X embedding_dim)\n",
    "# (batch_size X max_seq_len X embedding_dim) --->      Pack     ---> (batch_sum_seq_len X embedding_dim)\n",
    "# (batch_sum_seq_len X embedding_dim)        --->      LSTM     ---> (batch_sum_seq_len X hidden_dim)\n",
    "# (batch_sum_seq_len X hidden_dim)           --->    UnPack     ---> (batch_size X max_seq_len X hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bf0af6f",
   "metadata": {},
   "source": [
    "# Classification Task\n",
    "\n",
    "Learning is all about making assumptions. If we want to classify a new data example that we have never seen before we have to make some assumptions about which data examples are similar to each other. One natural way to express the classification task is via the **probabilistic question**: what is the most likely label given the features? In a classification task, we assume that:\n",
    "\n",
    "1. $X$ is a collection of input features examples with $d$ features: $X \\in \\mathcal{R}^{n \\times d}$ and $x_i = [x_{i1}, x_{i2}, \\ldots, x_{id}] \\in D$\n",
    "2. $Y$ is the corresponding set of labels, $Y \\in \\mathcal{R}^{m \\times 1}$. An label $y \\in Y$ is a integer number between 0 and 9 in our example. \n",
    "\n",
    "Then the classifier will output the prediction $\\hat{y}$ given by the expression:\n",
    "\\[ \\hat{y} = \\underset{y \\in [0, 9]}{\\text{arg max}} \\; p(y| x) \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021350bf",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier \n",
    "\n",
    "According to Bayes theorem (**conditional probability**), the classifier can be expressed as:\n",
    "\\[ \\hat{y} = \\underset{j \\in [0, n-1]}{\\text{arg max}} \\; p(y_j| x) = \\underset{y \\in [0, n-1]}{\\text{arg max}} \\; \\frac{p(x|y_j)p(y_j)}{p(x)}\\] where $n$ is the number of classes.\n",
    "\n",
    "Note that the denominator is the normalizing term $p(x)$ which does not depend on the value of the label y. As a result, we only need to worry about comparing the numerator across different values of y. Now, let us focus on $p(x | y)$ where $x = [x_1, x_2, \\ldots, x_d]$. Using the chain rule of probability: \n",
    "\\[ p(x | y) = p[x_1, x_2, \\ldots, x_d | y] = p(x_1 | y) \\cdot p(x_2 |x_1, y) \\ldots p(x_d | x_1, \\ldots, x_{d-1}, y)\\]\n",
    "Next, we assume that **the features are conditionally independent of each other, given the label**, then \n",
    "\\[ p(x | y) = p(x_1 | y) \\cdot p(x_2 |x_1, y) \\ldots p(x_d | x_1, \\ldots, x_{d-1}, y) = p(x_1 | y) \\cdot p(x_2 |y) \\ldots p(x_d | y) = \\prod_{i=1}^{d} p(x_i | y)\\]\n",
    "The predictor can be re-defined as:\n",
    "\\[ \\hat{y} = \\underset{j \\in [0, n-1]}{\\text{arg max}} \\; \\prod_{i=1}^{d} p(x_i | y)p(y_j)\\]\n",
    "If we can estimate $p(x_i = 1|y_j)$ for every i and j, and save its value in a $d\\times n$ matrix $P_{xy}[i, j]$, then we can also use this to estimate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e38a52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

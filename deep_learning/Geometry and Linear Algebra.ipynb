{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b73eaf8d",
   "metadata": {},
   "source": [
    "# Saving Memory\n",
    "\n",
    "\n",
    "Running operations can cause new memory to be allocated to host results. \n",
    "\n",
    "1. First, we do not want to run around allocating memory unnecessarily all the time. In machine learning, we might have hundreds of megabytes of parameters and update all of them multiple times per second. Typically, we will want to perform these updates **in place**.\n",
    "2. we might point at the same parameters from multiple variables. If we do not update in place, other references will still point to the old memory location, making it possible for parts of our code to inadvertently reference stale parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51c5bb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2232877748616\n",
      "2232877751896\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "y = torch.tensor([3.5])\n",
    "x = 1.2\n",
    "print(id(y))\n",
    "y = 1.2 + y\n",
    "print(id(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dff02b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(Z): 2232877750936\n",
      "id(Z): 2232877750936\n"
     ]
    }
   ],
   "source": [
    "# Fortunately, performing in-place operations is easy. We can assign the result of an operation \n",
    "# to a previously allocated array with slice notation\n",
    "Z = torch.zeros_like(y)\n",
    "print('id(Z):', id(Z))\n",
    "Z[:] = x + y\n",
    "print('id(Z):', id(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b37d372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2232877751896\n",
      "2232877751896\n"
     ]
    }
   ],
   "source": [
    "print(id(y))\n",
    "y[:] = 1.2 + y\n",
    "print(id(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d174f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2232877751896\n",
      "2232877751896\n"
     ]
    }
   ],
   "source": [
    "print(id(y))\n",
    "y += x\n",
    "print(id(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a138b64",
   "metadata": {},
   "source": [
    "# Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d483c96",
   "metadata": {},
   "source": [
    "## Scalar, Vector, Matrix and Tensor\n",
    "1. Formally, we call values consisting of just one numerical quantity **scalars** --> $x \\in \\mathbb{R}$\n",
    "\\begin{align}\n",
    "x = 2.5\n",
    "\\end{align}\n",
    "2. You can think of a **vector** as simply a list of scalar values. We call these values the elements (entries or components) of the vector. We work with vectors via one-dimensional tensors. In general tensors can have arbitrary lengths, subject to the memory limits of your machine. Extensive literature considers column vectors to be the default orientation of vectors, so does this book.  --> $X \\in \\mathbb{R}^n$ represents a vector x consists of n real-valued scalars.\n",
    "\\begin{equation}\n",
    "  X = \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "3. Just as vectors generalize scalars from order zero to order one, **matrices** generalize vectors from order one to order two. $A \\in \\mathbb{R}^{m \\times n}$ to express that the matrix A consists ofmrows and n columns of real-valued scalars\n",
    "\\begin{equation}\n",
    "A = \\begin{bmatrix}\n",
    "a_{11} & a_{12} & \\ldots & a_{1n} \\\\\n",
    "a_{21} & a_{22} & \\ldots & a_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots &\\vdots \\\\\n",
    "a_{m1} & a_{m2} & \\ldots & a_{mn}\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "- when a matrix has the same number of rows and columns, its shape becomes a square; thus, it is called a **square matrix**.\n",
    "- Sometimes, we want to flip the axes. When we exchange a matrixʼs rows and columns, the result is called the **transpose** of the matrix, which is denoted by $A^T$\n",
    "- a **symmetric matrix** A is equal to its transpose: $A = A^T$\n",
    "\n",
    "Matrices are useful data structures: they allow us to organize data that have **different modalities of variation**. For example, rows in our matrix might correspond to different houses (data examples), while columns might correspond to different attributes. \n",
    "\n",
    "Thus, although the default orientation of a single vector is a column vector, in a matrix that represents a tabular dataset, **it is more conventional to treat each data example as a row vector in the matrix.** And, as we will see in later chapters, this convention will enable common deep learning practices\n",
    "\n",
    "4. Just as vectors generalize scalars, and matrices generalize vectors, we can build data structures with even more axes. **Tensors** (“tensors” in this subsection refer to algebraic objects) give us a generic way of describing **n-dimensional arrays** with an arbitrary number of axes.\n",
    " - Vectors, for example, are **first-order** tensors, and matrices are **second-order** tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b042079",
   "metadata": {},
   "source": [
    "## Basic Properties of Tensor Arithmetic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8fb23e",
   "metadata": {},
   "source": [
    "### Hadamard product $A*B$\n",
    "\n",
    "<!-- $\\odot$ \\quad $\\oplus$ \\quad $\\otimes$ \\quad $\\ominus$ \\quad $\\oslash$ -->\n",
    "Specifically, elementwise multiplication of two matrices is called their Hadamard product (math notation $\\odot$). For example, $A, B \\in \\mathbb{R}^{m \\times n}$:\n",
    "\\begin{equation}\n",
    "B = \\begin{bmatrix}\n",
    "b_{11} & b_{12} & \\ldots & b_{1n} \\\\\n",
    "b_{21} & b_{22} & \\ldots & b_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots &\\vdots \\\\\n",
    "b_{m1} & b_{m2} & \\ldots & b_{mn}\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "A \\odot B = A * B= \\begin{bmatrix}\n",
    "a_{11}b_{11} &  a_{12}b_{12} & \\ldots & a_{1n}b_{1n} \\\\\n",
    "a_{21}b_{21} & a_{22}b_{22} & \\ldots & a_{2n}b_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots &\\vdots \\\\\n",
    "a_{m1}b_{m1} & a_{m2}b_{m2} & \\ldots & a_{mn}b_{mn}\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "### Dot Products $torch.dot(x, y)$\n",
    "\n",
    "Given two vectors $x, y \\in \\mathbb{R}^d$, their **dot product** $x^Ty$ or ($<x, y>$) is a sum over the products of the elements at the same position:\n",
    "\\begin{equation}\n",
    "x^Ty = \\sum_{i = 1}^d x_iy_i\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "213de306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "y = torch.ones(4, dtype=torch.float32)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a6b42d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([0., 1., 2., 3.])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ec73163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bad43610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bad32fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(x * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1a5f04",
   "metadata": {},
   "source": [
    "**Usage: weighted average** Dot products are useful in a wide range of contexts. For example, given some set of values, denoted by a vector $x \\in \\mathbb{R}^d$ and a set of weights denoted by $w \\in \\mathbb{R}^d$, the weighted sum of the values in $x$ according to the weights $w$ could be expressed as the dot product $x^Tw$. When the weights are non-negative and sum to one (i.e., $\\sum_{i=1}^{d}w_i = 1$), the dot product expresses a **weighted average**. After normalizing two vectors to have the unit length, the dot products express the cosine of the angle between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b3df49",
   "metadata": {},
   "source": [
    "### Matrix-Vector Products $torch.mv(A, x)$\n",
    "\n",
    "Recall the matrix $A \\in \\mathbb{R}^{m \\times n}$ and the vector $x \\in \\mathbb{R}^n$. The matrix-vector product $Ax$ is simply a column vector of length $m$, whose $i^{th}$ element is the dot product $a_i^Tx$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08ba31b",
   "metadata": {},
   "source": [
    "### Matrix-Matrix Multiplication $torch.mm(A, B)$\n",
    "\n",
    "Say that we have two matrices $A \\in \\mathbb{R}^{m x k}$ and $B \\in \\mathbb{R}^{k x n}$, $C = AB \\in mathbb{R}^{m x n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e9e66d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Norms\n",
    "\n",
    "Informally, the norm of a vector tells us how big a vector is. The notion of **size** under consideration here concerns not dimensionality but rather the magnitude of the components. \n",
    "\n",
    "\n",
    "In linear algebra, a vector norm is a function $f$ that maps a vector ($x, Y\\in \\mathbb{R}^n$) to a scalar $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$, satisfying a handful of properties:\n",
    "1. its norm also scales by the absolute value $\\alpha$ of the same constant factor: $f(\\alpha x) = |\\alpha| f(x)$. \n",
    "2. Triangle inequality: $f(x + y) \\leq f(x) + f(y)$\n",
    "3. the norm must be non-negative: $f(x) \\geq 0$\n",
    "\n",
    "\n",
    "You might notice that norms sound a lot like measures of distance:\n",
    "\n",
    "1. $L_2$ norm (Euclidean distance) of $x$ is the square root of the sum of the squares of the vector elements: \n",
    "\\begin{equation}\n",
    "L_2(x) = \\parallel x \\parallel_2 = \\sqrt{\\sum_{i=1}^{n}x_i^2}\n",
    "\\end{equation} \n",
    "In deep learning, we work more often with the squared L2 norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ff18afa",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In code, we can calculate the L2 norm of a vector as follows.\n",
    "u = torch.tensor([3.0, -4.0])\n",
    "torch.norm(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b397f003",
   "metadata": {
    "hidden": true
   },
   "source": [
    "2. $L_1$ norm is expressed as the sum of the absolute values of the vector elements: \n",
    "\\begin{equation}\n",
    "L_1(x) = \\parallel x \\parallel_1 = \\sum_{i=1}^{n}|x_i|\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc6cb709",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To calculate the L1 norm, we compose the absolute value function with a sum over the elements.\n",
    "torch.abs(u).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ad1997",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As compared with the L2 norm, it is less influenced by outliers.  Both the $L_2$ norm and the $L_1$ norm are special cases of the more general $L_p$ norm: \n",
    "\\begin{equation}L_p(x) = \\parallel x \\parallel_p = (\\sum_{i=1}^{n}|x_i|^p)^{1/p}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8685a3bd",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Analogous to $L_2$ norms of vectors, the Frobenius norm of a matrix $X \\in \\mathbb{R}^{m x n}$ is the square root of the sum of the squares of the matrix elements:\n",
    "\\begin{equation}\n",
    "\\parallel X \\parallel_F = \\sqrt{\\sum_{i=1}^m\\sum_{j=1}^nx_{ij}^2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4535ef9e",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoking the following function will calculate the Frobenius norm of a matrix.\n",
    "torch.norm(torch.ones((4, 9)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5a09f5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

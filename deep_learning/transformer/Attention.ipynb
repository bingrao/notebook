{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention的介绍\n",
    "\n",
    "<img src=\"images/nums_epoches.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "当我们去看张照片的时候，我们首先就是先去看整体，这里有车、有街道、还有很多的广告牌，不知道大家是否有感受到，当我开始描述这些的时候，其实就是我把注意力放在了这些上面了。那当我们想要跟深入了解这张图片的时候，我就要把注意力放的更聚焦。比如说，我想知道这是拍的哪里，那我可能会试着去看看广告牌上的文字，看这些文字是不是能给我一些启示。\n",
    "\n",
    "我们可能会试着把注意力放到不同的区域，那我们就能够得到更多的关于不同角度的信息。这些信息，正是我们希望在图像处理的时候希望得到的。\n",
    "\n",
    "\n",
    "\n",
    "比如说上面的这句话，“她正在吃一个绿色的苹果”，这里我们可以比较清楚的看到，“吃”和“苹果”有很强的联系，那我们就希望在处理吃这个单词的时候，能够在语义中，包含一定的苹果的信息，这样能够帮助我们更好的理解“吃”这个动作。“绿色的”和“苹果”也是一样的，attention的机制能够帮助我们在处理单个的token的时候，带有一定的上下文信息。这就像是一种“软性记忆”一样，帮助我们记住上下文中包含的信息。\n",
    "\n",
    "当我们看一篇文章的时候，其实也是类似的。我们从拿到一篇文章开始，首先关注的也只是一些关键性的词语，这些关键性的词语，就能够帮助我们快速的判断文章的内容和结构。这些场景就是我们在一些具体场景中对attention的应用。\n",
    "\n",
    "假设我们的时间序列： $X = \\{x_0, ..., x_n\\}$, 我们把它放到坐标轴上，这些点是我们从整体数据中采样出来的，这里有很多的噪音（noisy），我们想办法能不能通过一些方法，得到这些数据的更好的表示，从而能够使噪音减少。\n",
    "\n",
    "**Re-Weighting**: 让我们的这些点都包含一些其他点的信息，使得所有的数据能够更平滑一些. 我们定义这些**re-weighting**的参数为 $w_i = {w_{0i}, ..., w_{ni}}$, 我们使用这些weights就能够得到一个点的新的表示$y_i = w_i * X$.\n",
    "\n",
    "$$Y = \\{ w_i * x_i | w_i \\in W x_i \\in X\\}$$\n",
    "\n",
    "![](images/re_weight.png)\n",
    "\n",
    "这里的$w_i$， 其实本质上是对所有输入 X的一个总结，然后得到的一个对应的值。 \n",
    "\n",
    "\n",
    "Self-attention 是一个序列到序列的操作：一个向量的序列作为输入，一个向量的序列作为输出。 我们把输入的序列定义为 $x_1, x_2, ..., x_n$, 并且与它相关的输出向量是 $y_1, y_2, ..., y_n$. 这两个向量的维度都是$k$. 那么对于每一个点$x_i$, 我们都可以通过一个不同的权重值，来将它转化为一个新的序列，这个新的序列就可能是我们原始序列的一个更好的表示，这些 $w_i$ 就是一组attention的值.  它能够帮我们找到子序列和全局的attention的关系，也就是找到权重值 $w_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-attention就本质上是一种特殊的attention。attention机制能够帮我们找到子序列和全局的attention的关系，也就是找到权重值 $w_i$. self-attention对于attention的变化，其实就是寻找权重值$w_i$的过程不同。\n",
    "\n",
    "为了能够产生输出的向量$y_i$，self-attention其实是对所有的输入做了一个加权平均的操作: $y_i = \\sum^{n}_{j} w_{i,j} * x_j$. $n$代表整个序列的长度, 并且$\\sum^{n}_{j} w_{i,j} == 1$. 值得一提的是, 这里的$w_{i, j}$ 并不是一个需要神经网络学习的参数，它是来源于 $x_i$ 和 $x_j$的之间的计算的结果。 它们之间最简单的一种计算方式，就是使用点积的方式\n",
    "$$w'_{i,j} = {x_i}^{T} * x_j$$\n",
    "\n",
    "这个点积的输出的取值范围在负无穷和正无穷之间，所以我们要使用一个softmax把它映射到[0, 1]之间, 并且要确保它们对于整个序列而言的和为1\n",
    "$$w_{i, j} = softmax(w'_{i, j}) = \\frac{e^{w'_{i,j}}}{\\sum^n_{j} e^{w'_{i,j}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention 和 self-attention 的区别是什么 ？\n",
    "这里有几个重要的区别，可以帮助大家更好的区分在不同任务中的使用方法：\n",
    "\n",
    "1. 在神经网络中，通常来说你会有输入层（input），应用激活函数后的输出层（output），在RNN当中你会有状态（state）。如果attention (AT) 被应用在某一层的话，它更多的是被应用在输出或者是状态层上，而当我们使用self-attention（SA），这种注意力的机制更多的实在关注input上。\n",
    "\n",
    "2. Attention (AT) 经常被应用在从编码器（encoder）转换到解码器（decoder）。比如说，解码器的神经元会接受一些AT从编码层生成的输入信息。在这种情况下，AT连接的是两个不同的组件（component），编码器和解码器。但是如果我们用SA，它就不是关注的两个组件，它只是在关注你应用的那一个组件。那这里他就不会去关注解码器了，就比如说在Bert中，使用的情况，我们就没有解码器。\n",
    "\n",
    "3. SA可以在一个模型当中被多次的、独立的使用（比如说在Transformer中，使用了18次；在Bert当中使用12次）。但是，AT在一个模型当中经常只是被使用一次，并且起到连接两个组件的作用。\n",
    "\n",
    "4. SA比较擅长在一个序列当中，寻找不同部分之间的关系。比如说，在词法分析的过程中，能够帮助去理解不同词之间的关系。AT却更擅长寻找两个序列之间的关系，比如说在翻译任务当中，原始的文本和翻译后的文本。这里也要注意，在翻译任务重，SA也很擅长，比如说Transformer。\n",
    "\n",
    "5. AT可以连接两种不同的模态，比如说图片和文字。SA更多的是被应用在同一种模态上，但是如果一定要使用SA来做的话，也可以将不同的模态组合成一个序列，再使用SA。\n",
    "\n",
    "6. 对我来说，大部分情况，SA这种结构更加的general，在很多任务作为降维、特征表示、特征交叉等功能尝试着应用，很多时候效果都不错。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled-Dot Attention\n",
    "\n",
    "\\begin{equation}\n",
    "Attention(Q, K, V) = softmax(\\frac{QK^{T}}{\\sqrt{d_k}}) * V\n",
    "\\end{equation}\n",
    "\n",
    "其中 $Q \\in \\mathbb{R}^{n \\times d_k}, K \\in \\mathbb{R}^{m \\times d_k}, V \\in \\mathbb{R}^{m \\times d_k}$. softmax 则是在 m 的那一维进行归一化。而自注意力，则是对于同一个 $X \\in \\mathbb{R}^{n \\times d}$，通过不同的投影矩阵得到:\n",
    "\n",
    "\n",
    "\n",
    "$$Q = X * W_q, \\; \\text{where} \\; W_q \\in \\mathbb{R}^{d \\times R'} $$\n",
    "\n",
    "$$K = X * W_k, \\; \\text{where} \\; W_k\\in \\mathbb{R}^{d \\times R'} $$\n",
    "\n",
    "$$V = X * W_v, \\; \\text{where} \\; W_v \\in \\mathbb{R}^{d \\times R'} $$\n",
    "\n",
    "\n",
    "至于 Multi-Head Attention，则不过是 Attention 运算在不同的参数下重复多次然后将多个输出拼接起来，属于比较朴素的增强。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot-Product操作为什么要被缩放\n",
    "\n",
    "1. $d_k$ 较大时，向量内积容易取很大的值\n",
    "\n",
    "假设 query 和 key 向量中的元素都是相互独立的均值为 0，方差为 1 的随机变量，那么这两个向量的内积 $q^{T}k = \\sum_{i=1}^{d_k} q_i k_i$ 的均值为0, 方差为$d_k$\n",
    "\n",
    "当$d_k$较大的时候，点积的方差较大，不同的q与不同的k算出的对齐分数相差较大，有的远大于0，有的远小于0.\n",
    "\n",
    "2. 向量内积的值（对齐分数）较大时，softmax 函数梯度很小\n",
    "\n",
    "Softmax 公式为\n",
    "\\begin{equation}\n",
    "S(x_i) = \\frac{e^{x_i}}{\\sum_{j=0}^{n}e^{x_j}}\n",
    "\\end{equation}\n",
    "求偏导:\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial_{x_i}}S(x_i) = S(x_i)(1 - S(x_i)) \\\\\n",
    "\\frac{\\partial}{\\partial_{x_j}}S(x_i) = -S(x_i)S(x_j) \n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "这里$x_i$就是q的转置与k的点积运算: $x_i = q^{T}k$。所以当方差较大的时候，很有可能存在某个key，其与 query 计算出来的对齐分数远大于其他的 key 与该 query 算出的对齐分数。导致softmax的反向传播时候误差为0，梯度消失\n",
    "\n",
    "解决方法:\n",
    "\\begin{equation}\n",
    "score(q,k) = \\frac{q^Tk}{\\sqrt{d_k}}\n",
    "\\end{equation}\n",
    "\n",
    "这样对齐分数公式算的方差从$d_k$变为1，从而softmax反向运算的梯度不趋向于0\n",
    "\n",
    "- [Self-attention中dot-product操作为什么要被缩放](https://zhuanlan.zhihu.com/p/149903065)\n",
    "- [Properties of Dot Product of Random Vectors](https://zhuanlan.zhihu.com/p/436614439)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention必须有个Softmax吗?\n",
    "简单起见我们就没显式地写出 Attention 的缩放因子:\n",
    "\\begin{equation}\n",
    "Attention(Q, K, V) = softmax(QK^{T}) * V\n",
    "\\end{equation}\n",
    "\n",
    "本文我们主要关心 Self Attention 场景，所以为了介绍上的方便统一设 $Q, K, V \\in \\mathbb{R}^{n \\times d}$, 一般长序列场景下都有 $n >> d$\n",
    "\n",
    "\n",
    "### 摘掉Softmax\n",
    "\n",
    "$QK^T$这一步我们得到一个$n \\times n$的矩阵， 就是这一步决定了 Attention 的复杂度是$O(n^2)$; 如果没有 Softmax，那么就是三个矩阵连乘 $QK^TV$, ，而矩阵乘法是满足结合率的，所以我们可以先算 $K^TV$， 得到一个$d \\times d$的矩阵，然后再用$Q$左乘它, 由于$d << n$, 所以这样算大致的复杂度只是$O(n)$（就是$Q$左乘那一步占主导). **也就是说，去掉 Softmax 的 Attention 的复杂度可以降到最理想的线性级别$O(n)$, 这显然就是我们的终极追求：Linear Attention，复杂度为线性级别的 Attention**\n",
    "\n",
    "### 一般的定义\n",
    "\n",
    "Scaled-Dot Attention的定义等价修改为：\n",
    "\\begin{aligned}\n",
    "Attention(Q, K, V)_i &= Softmax(Q_i K*T)V \\\\\n",
    "&= \\sum_{j=1}^{n} score(q_i, k_j)  v_j \\\\\n",
    "&=\\sum_{j=1}^{n}\\frac{e^{q_i^Tk_j}}{\\sum_{j=1}^n e^{q_i^Tk_j}}v_j \\\\\n",
    "&= \\sum_{j=1}^{n} \\frac{sim(q_i, k_j)}{\\sum_{j=1}^{n}sim(q_i, k_j)} v_j\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "[线性Attention的探索：Attention必须有个Softmax吗?](https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247508324&idx=1&sn=0238864a68d94e7057574bee65315c6c&chksm=96ea7ee4a19df7f2332408475299041a876ff87ed0431bbb56736755ff3fc9fd5fbed29d6aa2&scene=21#wechat_redirect)\n",
    "\n",
    "[去掉 Attention 的 Softmax，复杂度降为 O (n)](https://wmathor.com/index.php/archives/1540/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "1. https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247508324&idx=1&sn=0238864a68d94e7057574bee65315c6c&chksm=96ea7ee4a19df7f2332408475299041a876ff87ed0431bbb56736755ff3fc9fd5fbed29d6aa2&scene=21#wechat_redirect\n",
    "\n",
    "\n",
    "2. https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247486960&idx=1&sn=1b4b9d7ec7a9f40fa8a9df6b6f53bbfb&chksm=96e9d270a19e5b668875392da1d1aaa28ffd0af17d44f7ee81c2754c78cc35edf2e35be2c6a1&token=1416940766&lang=zh_CN&scene=21#wechat_redirect\n",
    "\n",
    "3. https://mp.weixin.qq.com/s/zBUowlIU0JezWRnFBKYbug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

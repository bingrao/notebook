{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc6eb7ee",
   "metadata": {},
   "source": [
    "# What is probability?\n",
    "1. Frequentist: Probabilities represent long run frequencies of events\n",
    "2. Bayesian: \n",
    "  - Probability is used to quantify our uncertainty about something\n",
    "  - It can be used to model our uncertainty about events that do not have long term frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c41e0f7",
   "metadata": {},
   "source": [
    "## Discrete random variables\n",
    "\n",
    "For an event $A \\in \\mathcal{X}$, like \"it will rain tomorrow\".\n",
    " - **Probability mass function (PMF)**: p(A) means probability that event A is true\n",
    " - $0 \\leq p(A) \\leq 1$\n",
    " - $\\sum_{a \\in \\mathcal{X}}p(a) = 1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e8783d",
   "metadata": {},
   "source": [
    "### Joint Probability\n",
    "Given two events $A, B \\in \\mathcal{X}$, the probability of the joint event A and B:\n",
    "\\begin{align}\n",
    "p(A, B) = p(A \\cap B) = p(A | B)p(B) = p(B | A)p(A)\n",
    "\\end{align}\n",
    "\n",
    "### Marginal distribution (边缘分布)\n",
    "\n",
    "如果我们把每一个变量的概率分布称为一个概率分布，那么边缘分布就是若干个变量的概率加和所表现出的分布。\n",
    "\n",
    "Given a joint distribution, the marginal distribution is defined as:\n",
    "\\begin{equation}\n",
    "p(A) = \\sum_{b \\in \\mathcal{X}}p(A, B=b) = \\sum_{b \\in \\mathcal{X}}p(A|B=b)p(B=b)\n",
    "\\end{equation}\n",
    "\n",
    "举个例子，假设$p(B),p(C),p(A|B),p(A|C)$已知，求P(A):\n",
    "\\begin{equation}\n",
    "p(A) = \\sum_{b \\in [B, C]}p(A, B=b) = p(A, B) + p(A, C) = p(A|B)p(B) + P(A|C)p(C)\n",
    "\\end{equation}\n",
    "### Conditional Probability\n",
    "\n",
    "事件A在另外一个事件B已经发生条件下的发生概率。条件概率表示为P（A|B），读作“在B条件下A的概率”。\n",
    "\n",
    "We define the conditional probabiltity of an event A, given that event B is true:\n",
    "\\begin{equation}\n",
    "p(A|B) = \\frac{p(A, B)}{p(B)} \\; \\textrm{if} \\; p(B) \\; \\gt 0 \n",
    "\\end{equation}\n",
    "\n",
    "### Bayes Rule\n",
    "\n",
    "According to conditional and marginal probabilities, bayes rule is defined by: \n",
    "\\begin{equation}\n",
    "\\underbrace{p(X=\\mathcal{x}|Y=\\mathcal{y}) = \\frac{p(X=\\mathcal{x}, Y=\\mathcal{y})}{p(Y=\\mathcal{y})}}_{\\textrm{conditional probability}} \\; \\textrm{and} \\; \\underbrace{p(y) = \\sum_{x'}p(X=\\mathcal{x'})p(Y=\\mathcal{y} | X=\\mathcal{x'})}_{\\textrm{marginal probability}}  \\\\ \\Downarrow \\\\\n",
    "p(X=\\mathcal{x}|Y=\\mathcal{y}) = \\frac{p(X=\\mathcal{x}, Y=\\mathcal{y})}{\\sum_{x'}p(X=\\mathcal{x'})p(Y=\\mathcal{y} | X=\\mathcal{x'})}\n",
    "\\end{equation}\n",
    "\n",
    "### A medical diagnoise problem using Bayes Rule\n",
    "\n",
    "**Example**: Suppose you are a women in your 40s, and you decide to have medical test ($X = 1$) for breast cancer ($Y \\in [0, 1]$), which is called mammogram. If the test is positive, what's probability you have cancer $p(Y=1 | X=1)$? \n",
    "\n",
    "\\begin{proof}\n",
    "Assume that $p(X=1|Y=1) = 0.8, p(X=1|Y=0) = 0.1, p(Y=1) = 0.004$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "p(Y=1|X=1) &= \\frac{p(X=1, Y=1)}{p(X=1)} \\\\\n",
    "&= \\frac{p(X=1|Y=1)p(Y=1)}{p(X=1, Y=1) + p(X=1, Y=0)} \\\\\n",
    "&= \\frac{p(X=1|Y=1)p(Y=1)}{p(X=1|Y=1)p(Y=1) + p(X=1|Y=0)p(Y=0)} \\\\\n",
    "&= \\frac{0.8 * 0.004}{0.8 * 0.0004 + 0.1*0.0996} = 0.031 \n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\\end{proof}\n",
    "\n",
    "### Conditional Independence\n",
    "\n",
    "- If random variables $X$ and $Y$ are said to be independent if $p(X, Y) = p(X)p(Y)$, alternatively, $p(X|Y) = p(X), or p(Y|X) = p(Y)$, which is denoted by $X \\perp Y$\n",
    "- Conditional independent: $X \\perp Y | Z \\Leftrightarrow p(X, Y | Z) = p(X|Z)p(Y|Z)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8adceb4",
   "metadata": {},
   "source": [
    "## Continuous Random Variable\n",
    "\n",
    "- Probability density $p(x) (\\geq 0)$\n",
    "- The larger $p(x)$ for a variable $x$, the more likely that a variable around $x$ will be generated by this distribution. \n",
    "- The probability that $x$ falls into an interval $[a, b]$ can be computed as $\\int_{a}^{b} p(x) \\,dx$\n",
    "- The likelihood that any variable drawn from $p(x)$ must fall between postive and negative infinity $\\int_{-\\infty}^{\\infty}p(x)dx = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a849b07",
   "metadata": {},
   "source": [
    "## Summary stastistic Property\n",
    "\n",
    "Suppose that we are dealing with a random variables X. The distribution itself can be hard to interpret. It is often useful to be able to summarize the behavior of a random variable concisely. Numbers that help us capture the behavior of a random variable are called summary statistics. The most commonly encountered ones are the mean, the variance, and the standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bf6d5f",
   "metadata": {},
   "source": [
    "### Mean (a.k.a, expected value)\n",
    "\n",
    "- Discrete Random Variable: $\\mu = \\mathbb{E}[X] = \\sum_{\\mathcal{x} \\in X} x p(x)$. then the mean is given by the weighted average: sum the values times the probability that the random variable takes on that value\n",
    "- Continuous Random Variable: $\\mu = \\mathbb{E}[X] = \\int_{\\mathcal{x}}xp(x)dx$ \n",
    "\n",
    "Because they are helpful, let us summarize a few properties.\n",
    "- $\\mu_{aX+b} = a\\mu_{X} + b$\n",
    "- $\\mu_{X + Y} = \\mu_X + \\mu_Y$\n",
    "\n",
    "Means are useful for understanding the average behavior of a random variable, however the mean is not sufficient to even have a full intuitive understanding.\n",
    "\n",
    "### Variance (方差, \"spread\" of a distribution)\n",
    "\n",
    "This is a quantitative measure of how far a random variable deviates from the mean\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\textrm{var}[X] &= \\mathbb{E}[(X-\\mu)^2] = \\mathbb{E}[X^2 - 2\\mu X + \\mu^2] \\\\\n",
    "&= \\mathbb{E}[X^2] - \\mathbb{E}[2\\mu X] + \\mathbb{E}[\\mu^2] \\\\\n",
    "&= \\mathbb{E}[X^2] - 2\\mu \\mathbb{E}[X] + \\mathbb{E}[\\mu^2] \\\\\n",
    "&= \\mathbb{E}[X^2] - 2\\mu^2 + \\mu^2 = \\mathbb{E}[X^2] - \\mu^2\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Alternatively, proof as follow:\n",
    "\\begin{proof}\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\textrm{var}[X] &= \\mathbb{E}[(X-\\mu)^2] = \\int (x-\\mu)^2p(x)dx \\\\\n",
    "&= \\int x^2p(x) dx - \\int 2\\mu x p(x) dx + \\int \\mu^2 p(x)dx \\\\\n",
    "&= \\int x^2p(x) dx - 2\\mu \\int x p(x) dx + \\mu^2 \\int p(x)dx \\\\\n",
    "&= \\mathbb{E}[X^2] - 2\\mu \\mathbb{E}[X] + \\mu^2 * 1 \\\\\n",
    "&= \\mathbb{E}[X^2] - \\mu^2\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\\end{proof}\n",
    "\n",
    "\n",
    "We will list a few properties of variance below:\n",
    "- $Var[X] \\geq 0$; If X is constant, then $Var[X] = 0$\n",
    "- $Var[aX+b] = a^2Var[X]$\n",
    "- if X is independent Y, then $Var[X+Y] = Var[X] + Var[Y]$\n",
    "\n",
    "### Standard deviation\n",
    "\n",
    "- $\\delta_X = \\textrm{std}[X] = \\sqrt{\\textrm{var}[X]} $\n",
    "\n",
    "The properties we had for the variance can be restated for the standard deviation.\n",
    "- $\\delta_X \\geq 0$\n",
    "- $\\delta_{aX + b} = |a|\\delta_X$\n",
    "- if X is independent Y, \\delta_{X + Y} = \\sqrt_{\\delta^2_X + \\delta^2_Y}\n",
    "\n",
    "### Covariance (协方差)\n",
    "\n",
    "For two jointly distributed real-valued random variables X and Y with finite second moments, the covariance is defined as the expected value (or mean) of the product of their deviations from their individual expected values:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\delta_{XY} = \\textrm{cov}(X, Y) &= \\mathbb{E}[(X-\\mathbb{E}(X))(Y-\\mathbb{E}(Y))] = \\sum_{i, j}(x_i - \\mu_X)(y_j - \\mu_Y)P_{ij}\\\\\n",
    "&= \\mathbb{E}[(X-\\mu_X)(Y- \\mu_Y)] \\\\\n",
    "&= \\mathbb{E}[XY-X*\\mu_Y - Y*\\mu_X + \\mu_X*\\mu_Y] \\\\\n",
    "&= \\mathbb{E}[XY]-\\mathbb{E}[X*\\mu_Y] - \\mathbb{E}[Y*\\mu_X] + \\mathbb{E}[\\mu_X*\\mu_Y] \\\\\n",
    "&= \\mathbb{E}[XY]-\\mathbb{E}[X]*\\mu_Y - \\mathbb{E}[Y]*\\mu_X + \\mu_X*\\mu_Y \\\\\n",
    "&= \\mathbb{E}[XY]-\\mu_X*\\mu_Y - \\mu_Y*\\mu_X + \\mu_X*\\mu_Y \\\\\n",
    "&= \\mathbb{E}[XY]-\\mu_X*\\mu_Y \\\\\n",
    "&= \\mathbb{E}[XY]-\\mathbb{E}[X]\\mathbb{E}[Y]\n",
    "\\end{aligned}\\end{equation}\n",
    "\n",
    "If X is a d-dimnsional random vector, its covariance matrix is:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\textrm{cov}[X] &= \\mathbb{E}[(X-\\mathbb{E}[X])(X-\\mathbb{E}[X])^T] \\\\\n",
    "&= \\begin{bmatrix}\n",
    "\\color{red}{\\textrm{cov}[X_1, X_1]}  & \\textrm{cov}[X_1, X_2] & \\cdots &\\textrm{cov}[X_1, X_d]\\\\\n",
    "\\textrm{cov}[X_2, X_1] & \\color{red}{\\textrm{cov}[X_2, X_2]} & \\cdots &\\textrm{cov}[X_2, X_d]\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\textrm{cov}[X_d, X_1]& \\textrm{cov}[X_d, X_2] & \\cdots &\\color{red}{\\textrm{cov}[X_d, X_d]}\\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\color{red}{\\textrm{var}[X_1]}  & \\textrm{cov}[X_1, X_2] & \\cdots &\\textrm{cov}[X_1, X_d]\\\\\n",
    "\\textrm{cov}[X_2, X_1] & \\color{red}{\\textrm{var}[X_2]} & \\cdots &\\textrm{cov}[X_2, X_d]\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\textrm{cov}[X_d, X_1]& \\textrm{cov}[X_d, X_2] & \\cdots &\\color{red}{\\textrm{var}[X_d]}\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Let us see some properties of covariances:\n",
    "\n",
    "- $Cov[X, X] = Var[X]$\n",
    "- $Cov[aX + b, Y] = Cov[X, aY + b] = a Cov[X, Y]$\n",
    "- if X is independent of Y, then $Cov[X, Y] = 0$.\n",
    "- $Var[X + Y] = Var[X] + 2Cov[X, Y] + Var[Y]$\n",
    "\n",
    "### Correlation Coefficient\n",
    "\n",
    "- The (Pearson) correlation coefficient between X and Y measures the linear relationship, which is defined by: \n",
    "\\begin{equation}\n",
    "-1 \\leq \\rho_{XY} = \\textrm{corr}[X, Y] = \\frac{\\textrm{cov}[X, Y]}{\\sqrt{\\textrm{var}[X]\\textrm{var}[Y]}} = \\frac{Cov[X, Y]}{\\delta_X\\delta_Y}\\leq 1\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "consider X as any random variable, and Y = aX +b as any linear deterministic function of X. Then, one can compute that\n",
    "$$\\delta_Y = \\delta_{aX + b} = |a|\\delta_X$$\n",
    "$$Cov[X, Y] = Cov[X, aX+b] = a Cov[X, X] = a Var[X]$$\n",
    "and thus that:\n",
    "$$\\rho_{XY} = \\frac{aVar[x]}{|a|\\delta^2_X} = \\frac{a}{|a} = sign(a)$$\n",
    "Thus we see that the correlation is +1 for any a > 0, and -1 for any a < 0 illustrating that **correlation measures the degree and directionality the two random variables are related, not the scale that the variation takes.**\n",
    "\n",
    "\n",
    "Let us list a few properties of the correlation below.\n",
    "- \\$rho_{XX} = 1$\n",
    "- \\forall X, Y, a, b \\in \\mathcal{R}: \\rho[aX+b, Y] = \\rho[X, aY+b] = \\rho[X, Y]\n",
    "- If X and Y are independent with non-zero variance then $\\rho[X, Y] = 0$\n",
    "- Indeed if we think of norms as being related to standard deviations, and correlations as being cosines of angles, much of the intuition we have from geometry can be applied to thinking about random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba28b2f",
   "metadata": {},
   "source": [
    "## Common distributions (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1884d07d",
   "metadata": {},
   "source": [
    "### Bernoulli Distribution (Discrete)\n",
    "- Toss a coin only once\n",
    "- Let X be a binary variable $X \\in {0, 1}$\n",
    "\n",
    "- What's the probability of X that a toss shows up as \"head\":\n",
    "\\begin{equation}\n",
    "\\textrm{Ber}(x|\\theta) =  \\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t\t\\theta  & \\mbox{if } x = 0 \\\\\n",
    "\t\t1 - \\theta & \\mbox{if } x = 0\n",
    "\t\\end{array}\n",
    "\\right.\n",
    "\\end{equation}\n",
    "\n",
    "### Binomial Distribution (Discrete)\n",
    "\n",
    "\n",
    "### Multinomial Distribution (Discrete)\n",
    "\n",
    "### Gaussian Distribution (Continuous)\n",
    "\n",
    "### Laplace Distribution  (Continuous)\n",
    "\n",
    "### Beta Distribution  (Continuous)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e2260d",
   "metadata": {},
   "source": [
    "# Bayesian Theory\n",
    "\n",
    "所谓的贝叶斯方法源于他生前为解决一个“逆概”问题写的一篇文章，而这篇文章是在他死后才由他的一位朋友发表出来的。在贝叶斯写这篇文章之前，人们已经能够计算“正向概率”，如“假设袋子里面有N个白球，M个黑球，你伸手进去摸一把，摸出黑球的概率是多大”。而一个自然而然的问题是反过来：“如果我们事先并不知道袋子里面黑白球的比例，而是闭着眼睛摸出一个（或好几个）球，观察这些取出来的球的颜色之后，那么我们可以就此对袋子里面的黑白球的比例作出什么样的推测”。这个问题，就是所谓的逆概问题。\n",
    "\n",
    "\n",
    "贝叶斯是机器学习的核心方法之一。这背后的深刻原因在于，现实世界本身就是不确定的，人类的观察能力是有局限性的，我们日常所观察到的只是事物表面上的结果，沿用刚才那个袋子里面取球的比方，我们往往只能知道从里面取出来的球是什么颜色，而并不能直接看到袋子里面实际的情况。这个时候，我们就需要提供一个假设（hypothesis）。所谓假设，当然就是不确定的（可能是有限个，也可能是无限多种），为了确定哪个假设是正确的，我们需要做两件事情：1、算出各种不同猜测的可能性大小。2、算出最靠谱的猜测是什么。第一个就是计算特定猜测的后验概率（Posterior），对于连续的猜测空间则是计算猜测的概率密度函数。第二个则是所谓的模型比较，模型比较如果不考虑先验概率（Prior）的话就是最大似然方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aeffc6",
   "metadata": {},
   "source": [
    "## Generative Classifer\n",
    "\n",
    "对于一个训练数据集X，如何判断他属于哪个类型？ \n",
    "\n",
    "通用的方法就是求解在已经数据集X的情况下，求出每个分类针对于该数据集的概率，概率最大的就是最大的可能性。 其中$\\theta$是关于这个模块的参数。\n",
    "\n",
    "Given a dataset $X$, the probability of (Y=c) for this dataset is defined using the class conditional density ($p(X|Y=c)$) and the class prior $p(Y=c)$:\n",
    "\\begin{equation}\n",
    "\\label{eq:baysian_rule}\n",
    "\\begin{aligned}\n",
    "p(Y=c|X, \\theta) &= \\frac{p(X, Y=c, \\theta)}{p(X, \\theta)} \\\\\n",
    "& = \\frac{p(X|Y=c, \\theta)p(Y=c, \\theta)}{\\sum_{c'}p(Y=c', \\theta)p(X|Y=c', \\theta)} \\\\\n",
    "&\\propto p(X|Y=c, \\theta)p(Y=c, \\theta)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "- $p(Y=c | X, \\theta)$: Posterior probability (后验概率)\n",
    "- $f(\\theta) = p(X|Y=c, \\theta)$: likelihood probability, we also call this function likelihood function (似然函数)\n",
    "- $p(Y=c, \\theta)$: prior probability (先验概率)\n",
    "- $p(X, \\theta)$: Normalized factor(证据因子)\n",
    "\n",
    "**Solution:** the posterior equals to the likelihood times the prior, up to a constant. \n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\textrm{Posterior (后验概率)} &= \\frac{\\textrm{prior (先验概率)} \\times \\textrm{likelihood (似然函数)}}{\\textrm{证据因子}} \\\\\n",
    "&\\propto \\textrm{prior (先验概率)} \\times \\textrm{likelihood (似然函数)}\n",
    "\\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2d871c",
   "metadata": {},
   "source": [
    "## Example: Number Game (from Josh Tenenbaum's PhD thesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df41de40",
   "metadata": {},
   "source": [
    "Inferring abstract patterns from sequence of integers\n",
    "\n",
    "**Problem:** There is a group of hypotheses $\\mathcal{H}$: {'Primer number', 'a number between 1 and 10', $\\ldots$}. In this task, a set of positive examples (training datasets) $D = \\{x_1, x_2, \\ldots, x_n\\}$ drawn from a reasonable arithmetical concept $h \\in \\mathcal{H}$. Finally, I would ask that whether a new test case $x$ belong to $h$ (classifying $x$) \n",
    "- **Posterior predictive distribution**: what's probability of that $X \\in h$ given the dataset $D$: $p(x \\in h |D, \\theta)$.\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "p(x \\in h | D, \\theta) & = \\frac{p(x \\in h, D, \\theta)}{p(D, \\theta)} \\\\\n",
    "&= \\frac{p(D|x \\in h, \\theta)p(x \\in h, \\theta)}{p(D, \\theta)} \\\\\n",
    "& = \\frac{p(D|x \\in h, \\theta)p(x \\in h, \\theta)}{\\sum_{h' \\in \\mathcal{H}}p(D, D \\in h', \\theta)} \\\\\n",
    "&\\propto p(D|x \\in h, \\theta)p(x \\in h, \\theta)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "**Example**\n",
    "- For simplicity, assume all numbers are integers between 1 and 100\n",
    "- $D = \\{2, 8, 16, 64\\}$ which are samples from IID (independent and identically distributed) datasets\n",
    "\n",
    "**Solution**\n",
    "\n",
    "1. **Hypothesis space of concepts $\\mathcal{H}$**\n",
    "\n",
    "Usually, the models favors the simplest hypothesis consistent with the data (Occam's razor). In this case, we choose $\\mathcal{H} = \\{h_1, h_2, h_3\\}$\n",
    "- \"powers of two\", that's $h_1 = h_{two} = \\{2, 4, 8, 16, 32, 64\\}$, where $|h_{1}| = 6$ \n",
    "- \"even numbers\", that's $h_2 = h_{even} = \\{2, 4, 6, \\ldots, 100\\}$, where $|h_{2}|=50$\n",
    "- \"powers of two except 32\", that's $h_3 = h_{two-32} = \\{2, 4, 8, 16, 64\\}$, where $|h_{3}| = 5$ \n",
    "\n",
    "2. **Prior p(h)**\n",
    "\n",
    "Usually, prior probability captures the background knowledge, domain knowledge, pre-existing biases. In our case, it looks like $h_{two-32}$ is much better fit than $h_{two}$, however, the former seems \"conceptually unnatural\". Therefore, we can capture such intuition by assigning lower prior probability to unnatural concepts. \n",
    "\\[ p(\\mathcal{H}) = \\begin{bmatrix} p(h_1) = 0.19, \\\\ p(h_2) = 0.8, \\\\ p(h_3) = 0.01 \\end{bmatrix}\\]\n",
    "\n",
    "\n",
    "3. **Likelihood p(D|h)**\n",
    "\n",
    "In this case, we define likelihood function (statistical information in examples) as follows:\n",
    "\\[ p(D | h) = [\\frac{1}{size(h)}]^n = [\\frac{1}{|h|}]^n \\; \\textrm{if} x_1, \\ldots, x_n \\in h\\] \n",
    "where $n$ means the total number of elements in $D$. Smaller hypotheses receive greater likelihood, and exponentially more so as n increases.\n",
    "\n",
    "Therefore, we can get likelihood probability for $h_1$:  \n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "p(D|h_1) &= p(x_1|h_1) \\times p(x_2|h_1) \\times p(x_3|h_1) \\times p(x_4|h_1) \\;\\;\\; (\\textrm{data from IID}) \\\\\n",
    "&= \\frac{1}{6} \\times \\frac{1}{6} \\times \\frac{1}{6} \\times \\frac{1}{6} \\\\\n",
    "&= [\\frac{1}{|h_1|}]^4  \\;\\;\\; (|h_1| = 6)  \\\\\n",
    "&= [\\frac{1}{6}]^4\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "Similarly, we can compute likelihood probabilities for $h_2$ and $h_3$:\n",
    "\\[p(D|h_2) = [\\frac{1}{|h_2|}]^4 = [\\frac{1}{50}]^4\\]\n",
    "\\[p(D|h_3) = [\\frac{1}{|h_3|}]^4 = [\\frac{1}{5}]^4\\]\n",
    "\n",
    "4. **Posterior p(h|D)**\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "p(x \\in h_1 | D) & = \\frac{p(x \\in h_1, D)}{p(D)} \\\\\n",
    "&= \\frac{p(D|x \\in h_1)p(x \\in h_1)}{p(D)} \\\\\n",
    "& = \\frac{p(D|x \\in h_1)p(x \\in h_1)}{\\sum_{h' \\in \\mathcal{H}}p(D, D \\in h')} \\\\\n",
    "&\\propto p(D|x \\in h_1, \\theta)p(x \\in h_1) \\\\\n",
    "&= p(D|h_1)p(h_1)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331a1d02",
   "metadata": {},
   "source": [
    "## Approaches to Estimate Parameters from IID data\n",
    "\n",
    "Usually, the posterior is simply the liklihood times the prior, and then normalized:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "p(h|D, \\theta) &= \\frac{p(D, h, \\theta)}{p(D, \\theta)} \\\\\n",
    "& = \\frac{p(D|h, \\theta)p(h, \\theta)}{\\sum_{h'}p(Y=h', \\theta)p(D|h', \\theta)} \\\\\n",
    "&\\propto p(D|h, \\theta)p(h, \\theta)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Maximum Likelihood Estimation (MLE) and Maximum A Posteriori (MAP), are both a method for estimating some variable in the setting of probability distributions or graphical models. They are similar, as they compute a single estimate, instead of a full distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a6f6f5",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimation (MLE)\n",
    "MLE, as we, who have already indulge ourselves in Machine Learning, would be familiar with this method. This is the concept that when working with a probabilistic model with unknown parameters, the parameters which make the data have the highest probability are the most likely ones.\n",
    "\n",
    "\n",
    "Suppose that we have a model with parameters $\\theta$ and a collection of data examples X. For concreteness, we can imagine\n",
    "that $\\theta$ is a single value representing the probability that a coin comes up heads when flipped, and X is a sequence of independent coin flips. We will look at this example in depth later.\n",
    "\n",
    "If we want to find the most likely value for the parameters of our model, that means we want to find:\n",
    "\\[ \\text{arg max} P(\\theta | X) \\]\n",
    "\n",
    "By Bayesʼ rule, this is the same thing as\n",
    "\\[ \\text{arg max} P(\\theta | X) = \\text{arg max} \\frac{P(X|\\theta)P(\\theta)}{P(X)} \\] where\n",
    "1. Evidence $P(X)$ is a a parameter agnostic probability of generating the data, does not depend on $\\theta$ at all. so it can be dropped without changing the best choice of $\\theta$\n",
    "2. Prior $P(\\theta)$ does not depend on $\\theta$, that's uninformative prior. \n",
    "3. Likelihood $P(X | \\theta)$ is the probability of the data given the parameters.\n",
    "\n",
    "Given above two assumptions, we see that our application of Bayesʼ rule shows that our best choice of $\\theta$ is the maximum likelihood estimate for $\\theta$:\n",
    "\\[ \\hat{\\theta} = \\underset{\\theta}{\\text{arg max}} P(X | \\theta)\\]\n",
    "\n",
    "Sometimes, we even use it without knowing it. Take for example, when fitting a Gaussian to our dataset, we immediately take the sample mean and sample variance, and use it as the parameter of our Gaussian. This is MLE, as, if we take the derivative of the Gaussian function with respect to the mean and variance, and maximizing it (i.e. setting the derivative to zero), what we get is functions that are calculating sample mean and sample variance. Another example, most of the optimization in Machine Learning and Deep Learning (neural net, etc), could be interpreted as MLE.\n",
    "\n",
    "<!-- \\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\hat{h}^{MLE} &= \\underset{h \\in \\mathcal{H}}{\\textrm{argmax}} [p(h|D, \\theta)] \\\\\n",
    "&= \\underset{h \\in \\mathcal{H}}{\\textrm{argmax}} [\\frac{p(D|h, \\theta)p(h, \\theta)}{\\sum_{h'}p(Y=h', \\theta)p(D|h', \\theta)}] \\\\\n",
    "&\\propto \\underset{h \\in \\mathcal{H}}{\\textrm{argmax}}[\\log(p(D|h, \\theta)]\\;\\;\\;(\\text{If we ignore the prior} )\n",
    "\\end{aligned}\n",
    "\\end{equation} -->\n",
    "\n",
    "Speaking in more abstract term, let’s say we have a likelihood function $P(X|\\theta)$. Then, the MLE for $\\theta$, the parameter we want to infer, is:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\theta_{MLE} &= \\underset{\\theta}{\\textrm{argmax}} P(X|\\theta) \\\\\n",
    "&= \\underset{\\theta}{\\textrm{argmax}}\\prod_{i} P(x_i|\\theta) \\;\\;\\; (x_i \\in X \\;\\text{sampled from IID datasets})\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "As taking a product of some numbers less than 1 ($ 0 \\leq P(x_i|\\theta) \\leq 1$) would approaching 0 as the number of those numbers goes to infinity, it would be not practical to compute, because of computation underflow. Hence, we will instead work in the **log** space, as logarithm is monotonically increasing, so maximizing a function is equal to maximizing the log of that function.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\theta_{MLE} &= \\underset{\\theta}{\\textrm{argmax}} \\log P(X|\\theta) \\\\\n",
    "&= \\underset{\\theta}{\\textrm{argmax}} \\log  \\prod_{i} P(x_i|\\theta) \\;\\;\\; (x_i \\in X \\;\\text{sampled from IID datasets}) \\\\\n",
    "&= \\underset{\\theta}{\\textrm{argmax}} \\sum_{i} \\log P(x_i|\\theta)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "To use this framework, we just need to derive the log likelihood of our model, then maximizing it with regard of  $\\theta$ using our favorite optimization algorithm like Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acd62b7",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Maximum A Posteriori (MAP)\n",
    "\n",
    "MAP usually comes up in **Bayesian** setting. Because, as the name suggests, it works on a posterior distribution, not only the likelihood. When we have enough data, the posterior becomes peaked on a single concept.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\hat{h}^{MAP} &= \\underset{h \\in \\mathcal{H}}{\\textrm{argmax}} [p(h|D, \\theta)] \\\\\n",
    "&= \\underset{h \\in \\mathcal{H}}{\\textrm{argmax}} [\\frac{p(D|h, \\theta)p(h, \\theta)}{\\sum_{h'}p(Y=h', \\theta)p(D|h', \\theta)}] \\\\\n",
    "&\\propto \\underset{h \\in \\mathcal{H}}{\\textrm{argmax}}[\\log(p(D|h, \\theta)p(h, \\theta))] \\\\\n",
    "&= \\underset{h \\in \\mathcal{H}}{\\textrm{argmax}}[\\log p(D|h, \\theta)  + \\log p(h, \\theta)]\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "在众多的$\\mathcal{H}$中，哪个posterior probability是最大的，那么当前这个数据集最属于那个$h$\n",
    "\n",
    "\n",
    "Recall, with Bayes’ rule, we could get the posterior as a product of likelihood and prior:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "P(\\theta|X) &= \\frac{P(X|\\theta)P(\\theta)}{P(X)} \\\\\n",
    "&\\propto P(X|\\theta)P(\\theta)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "We are ignoring the **normalizing constant** as we are strictly speaking about optimization here, so proportionality is sufficient. If we replace the likelihood in the MLE formula above with the posterior, we get:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\theta_{MAP} &= \\underset{\\theta}{\\textrm{argmax}}  \\log P(X|\\theta)P(\\theta) \\\\\n",
    "&= \\underset{\\theta}{\\textrm{argmax}} [\\log P(X|\\theta) + \\log P(\\theta)] \\\\\n",
    "&= \\underset{\\theta}{\\textrm{argmax}} [\\log \\prod_{i}{P(x_i | \\theta)} + \\log P(\\theta)]  \\;\\;\\; (x_i \\in X \\;\\text{sampled from IID datasets})\\\\\n",
    "&= \\underset{\\theta}{\\textrm{argmax}} \\sum_{i} \\log P(x_i | \\theta) + \\underset{\\theta}{\\textrm{argmax}}  \\log P(\\theta)\n",
    "\\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b571d1",
   "metadata": {},
   "source": [
    "### Comparison between MLE and MAP\n",
    "Comparing both MLE and MAP equation, the only thing differs is the inclusion of prior $P(\\theta)$ in MAP, otherwise they are identical. What it means is that, the likelihood is now weighted with some weight coming from the prior.\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\theta_{MLE} &= \\underset{\\theta}{\\textrm{argmax}} \\sum_{i} \\log P(x_i | \\theta) \\\\\n",
    "\\theta_{MAP} &= \\underset{\\theta}{\\textrm{argmax}} \\sum_{i} \\log P(x_i | \\theta) + \\underset{\\theta}{\\textrm{argmax}}  \\log P(\\theta)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "There are several options for prior distribution:\n",
    "\n",
    "- **Uniform Prior:** This means, we assign equal weights everywhere, on all possible values of the $\\theta$. The implication is that the likelihood equivalently weighted by some constants. Being constant, we could be ignored from our MAP equation, as it will not contribute to the maximization. $\\theta_{MLE} = \\theta_{MAP}$\n",
    "- **Gaussian Prior:** Depending on the region of the distribution, the probability is high or low, never always the same.\n",
    "- Others, like Beta distribution etc. \n",
    "\n",
    "What we could conclude then, **is that MLE is a special case of MAP, where the prior is uniform!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c539fc",
   "metadata": {},
   "source": [
    "### Numerical Optimization and the Negative Log-Likelihood\n",
    "Using $\\log$ space in MLE and MAP with following considerations\n",
    "1. Notice that if we make the assumption that all the data examples are independent, we can no longer practically consider the likelihood itself as it is a product of many probabilities. For example, $p(x) = 0.5$, then product of $(1/2)^1000000000$ is far below machine precision. We cannot work with that directly. We can leverage log-likelihood to overcome this issue. \n",
    "\\[ \\log((1/2)^1000000000) = 1000000000*\\log(1/2) \\approx -301029995.6...\\]\n",
    "This number fits perfectly within even a single precision 32-bit float. Thus, we should consider the log-likelihood, which is\n",
    "\\[ \\log P[X|\\theta]\\]\n",
    "Since the function $x \\rightarrow \\log(x)$ is increasing, maximizing the likelihood (e.g. $\\underset{\\theta}{\\text{arg max} P(X|\\theta)}$) is the same thing as maximizing the log-likelihood  (e.g. $\\underset{\\theta}{\\text{arg max} \\log P(X|\\theta)}$) .\n",
    "We often work with loss functions, where we wish to minimize the loss. We may turn **maximum likelihood** into the **minimization** of a loss by taking $-\\log(P(X|\\theta))$, which is the **negative log-likelihood**. \n",
    "\n",
    "To illustrate this, consider the coin flipping problem from before, and pretend that we do not know the closed form solution. We may compute that:\n",
    "\\[ -\\log (P(X | \\theta)) = -\\log(\\theta^{n_H}(1-\\theta)^{n_T}) = -(n_H\\log(\\theta)+n_T\\log(1-\\theta)) \\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c03eedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.5017, requires_grad=True), 0.9970550284664874)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This can be written into code, and freely optimized even for billions of coin flips\n",
    "\n",
    "import torch\n",
    "# Set up our data\n",
    "n_H = 8675309\n",
    "n_T = 25624\n",
    "# Initialize our paramteres\n",
    "theta = torch.tensor(0.5, requires_grad=True)\n",
    "\n",
    "# Perform gradient descent\n",
    "lr = 0.00000000001\n",
    "for iter in range(10):\n",
    "    loss = -(n_H * torch.log(theta) + n_T * torch.log(1 - theta))\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        theta -= lr * theta.grad\n",
    "    theta.grad.zero_()\n",
    "# Check output\n",
    "theta, n_H / (n_H + n_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8803b96",
   "metadata": {},
   "source": [
    "2. The second reason we consider the log-likelihood is the simplified application of calculus rules. As discussed above, due to independence assumptions, most probabilities we encounter in machine learning are products of individual probabilities.\n",
    "\\[ P(X | \\theta) = p(x_1|\\theta)p(x_2|\\theta)\\ldotsp(x_n|\\theta)\\]\n",
    "This means that if we directly apply the product rule to compute a derivative we get a signficant computations, which require $n(n-1)$ multiplications, along with $n-1$ additions. So it is proportional to **quadratic time** in the inputs! For the negative log-likelihood we have instead:\n",
    "\\[ -\\log P(X | \\theta) = -(\\log(p(x_1|\\theta)) + \\log(p(x_2|\\theta)) + \\ldots+ \\log(p(x_n|\\theta)))\\]\n",
    "which then gives\n",
    "\\[ -\\frac{\\partial}{\\partial \\theta} \\log(P(X | \\theta)) = \\frac{1}{P(x_1 | \\theta)}\\frac{\\partial}{\\partial \\theta} P(x_1 | \\theta) + \\ldots +  \\frac{1}{P(x_n | \\theta)}\\frac{\\partial}{\\partial \\theta} P(x_n | \\theta)  \\]\n",
    "This requires only n divides and n - 1 sums, and thus is **linear time** in the inputs.\n",
    "3. The third and final reason to consider the negative log-likelihood is the relationship to information theory. This is a rigorous mathematical theory which gives a way to measure the degree of information or randomness in a random variable. The key object of study in that field is the entropy which is: $H(p) = - \\sum_{i}p_i\\log_2(p_i)$, which measures the randomness of a source. Notice that this is nothing more than the average -log probability, and thus if we take our negative log-likelihood and divide by the number of data examples, we get a relative of entropy known as **cross-entropy**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7ef486",
   "metadata": {},
   "source": [
    "### Example 1: Bernoulli model\n",
    "\n",
    "We perform following steps to estimate the parameters of the model: \n",
    "\n",
    "1. We observed $N$ IID coin tossing: $D = \\{x_1, x_2,  \\ldots, x_n\\} \\;\\text{where}\\; x_i \\in \\{0, 1\\}$, For example, $D = \\{1, 0, 1, \\ldots, 0\\}$,\n",
    "2. The model for an instance $x \\in D$ is defined as follow:\n",
    "\\begin{equation}\n",
    "P(x|\\theta) = \\textrm{Ber}(x|\\theta) =  \\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t\t\\theta  & \\mbox{if } x = 0 \\\\\n",
    "\t\t1 - \\theta & \\mbox{if } x = 0\n",
    "\t\\end{array}\n",
    "\\right. \\, \\Rightarrow \\, P(x|\\theta) = \\theta^x(1-\\theta)^{1-x}\n",
    "\\end{equation}\n",
    "3. **MLE:** We need to maximize the objective function - log likelihood of dataset $D$:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "L(\\theta; D) &= \\log P(D|\\theta) \\\\\n",
    "&= \\log \\prod_{i}P(x_i|\\theta) = \\log \\prod_{i} \\theta^{x_i}(1-\\theta)^{1-x_i} \\\\\n",
    "&= \\log \\theta^{\\sum_{i}x_i}(1-\\theta)^{\\sum_i(1-x_i)} \\\\\n",
    "&= \\log \\theta^{n_h}(1-\\theta)^{n_t} \\\\\n",
    "&= n_h log \\theta + n_t \\log (1 - \\theta) \\\\\n",
    "&= n_h log \\theta + (N - n_h) \\log (1 - \\theta)\n",
    "\\end{aligned}\n",
    "\\,\\Rightarrow\\, s.t. \\underset{\\theta}{\\textrm{argmax}} L(\\theta; D)\n",
    "\\end{equation}\n",
    "\n",
    "4. In order to get max value of this objective function, we take derivatives of $\\theta$:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L(\\theta; D)}{\\partial \\theta} = \\frac{n_h}{\\theta} - \\frac{N-n_h}{1 - \\theta} = 0 \\\\ \\Downarrow \\\\ \\hat{\\theta}_{MLE} = \\frac{n_h}{N} = \\frac{n_h}{n_h + n_t} \\; \\text{or} \\; \\hat{\\theta}_{MLE} = \\frac{1}{N}\\sum_{i} x_i \n",
    "\\end{equation}\n",
    "\n",
    "**Overfiting Problem:** What if we tossed too few times so that we saw zero head? $\\hat{\\theta}_{MLE} = 0$. To overcome this problem, we make it more formal:\n",
    "\\begin{equation}\n",
    "\\hat{\\theta}_{MLE} = \\frac{n_h + n'}{n_h + n_t + n'} \\; \\text{where} \\; n' \\text{ is known as the pseudo (imaginary) count.}  \n",
    "\\end{equation}\n",
    "\n",
    "5. ***MAP:*** If we take Beta distribution as the prior of model parameter $\\theta$:\n",
    "\\begin{equation}\n",
    "P(\\theta; \\alpha, \\beta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\theta^{\\alpha - 1}(1-\\theta)^{\\beta - 1} = B(\\alpha, \\beta)\\theta^{\\alpha - 1}(1-\\theta)^{\\beta - 1}\n",
    "\\end{equation}\n",
    "Then, according Bayesian rule:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "P(\\theta|D) &\\propto P(D|\\theta)P(\\theta) \\\\\n",
    "&= \\underbrace{\\theta^{n_h}(1-\\theta)^{n_t}}_{P(D|\\theta)} \\times \\underbrace{B(\\alpha, \\beta)\\theta^{\\alpha - 1}(1-\\theta)^{\\beta - 1}}_{P(\\theta)} \\\\\n",
    "&= B(\\alpha, \\beta)\\theta^{n_h + \\alpha - 1}(1-\\theta)^{n_t + \\beta - 1}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "Maximum a posterior (MAP) estimation:\n",
    "\\begin{equation}\n",
    "\\theta_{MAP} = \\underset{\\theta}{\\textrm{argmax}}  \\log P(\\theta|D) \\propto \\underset{\\theta}{\\textrm{argmax}}  \\log P(D|\\theta)P(\\theta)\n",
    "\\end{equation}\n",
    "According to step 4 to get the corresponding derivatives of $\\theta$:\n",
    "\\begin{equation}\n",
    "\\hat{\\theta_{MAP}} = \\hat{\\theta_{Bays}} = \\int \\theta p(\\theta|D)d\\theta = C\\int \\theta \\times \\theta^{n_h + \\alpha - 1}(1-\\theta)^{n_t + \\beta - 1}d\\theta = \\frac{n_h + \\alpha}{N + \\alpha + \\beta}\n",
    "\\end{equation}\n",
    "where $A = \\alpha + \\beta$ is prior strength, which can be interoperated as the size of an imaginary data set from which we obtain the **pseudo-counts**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3b332c",
   "metadata": {},
   "source": [
    "### Example 2: Univariate Normal (Gaussian)\n",
    "We perform following steps to estimate the parameters of the model: \n",
    "\n",
    "1. We observed $N$ IID coin tossing: $D = \\{x_1, x_2,  \\ldots, x_n\\}$, For example, $D = \\{-0.1, 10, 1, \\ldots, 3\\}$,\n",
    "2. The model with parameter $\\mu, \\delta$ for an instance $x \\in D$ is defined as follow:\n",
    "\\begin{equation}\n",
    "P(x|\\mu, \\delta) = \\frac{1}{\\sqrt{2\\pi \\delta^2}}e^{-\\frac{(x-\\mu)^2}{2\\delta^2}}\n",
    "\\end{equation}\n",
    "3. **MLE:** We need to maximize the objective function - log likelihood of dataset $D$:\n",
    " \\begin{equation}\n",
    "\\begin{aligned}\n",
    "L(\\mu, \\delta; D) &= log P(D|\\mu, \\delta) \\\\\n",
    "&= \\log \\prod_{i}P(x_i|\\theta) \\\\\n",
    "&= \\sum_{i} \\log P(x_i | \\theta) \\\\\n",
    "&= \\sum_{i} \\log \\frac{1}{\\sqrt{2\\pi \\delta^2}}e^{-\\frac{(x_i-\\mu)^2}{2\\delta^2}} \\\\\n",
    "&= \\sum_{i}(-\\frac{1}{2} \\log (2\\pi \\delta^2) - \\frac{(x_i-\\mu)^2}{2\\delta^2}) \\\\\n",
    "&= -\\frac{N}{2} \\log (2\\pi \\delta^2) - \\sum_{i}(\\frac{(x_i-\\mu)^2}{2\\delta^2})\n",
    "\\end{aligned}\n",
    "\\,\\Rightarrow\\, s.t. \\underset{\\theta}{\\textrm{argmax}} L(\\theta; D)\n",
    "\\end{equation}\n",
    "\n",
    "4. In order to get max value of this objective function, we take derivatives of $\\mu, \\delta^2$:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial \\mu} = \\frac{\\sum_{i}(x_i - \\mu)}{\\delta^2} = 0 \\Rightarrow \\mu_{MLE} = \\frac{\\sum_{i}{x_i}}{N} \\\\\n",
    "\\frac{\\partial L}{\\partial \\delta^2} = -\\frac{N}{2\\delta^2} + \\frac{1}{2\\delta^2}\\sum_{i}(x_i - \\mu)^ = 0 \\Rightarrow \\delta^2_{MLE} = \\frac{1}{N}\\sum_{i}(x_i - \\mu_{MLE})^2\n",
    "\\end{equation}\n",
    "\n",
    "5. **MAP:** Similarly, we can assume normal prior for model parameter $\\mu$: \n",
    "\\[ P(u) = \\frac{1}{\\sqrt{2\\pi \\tau^2}}e^{-\\frac{(x-\\mu)^2}{2\\tau^2}} \\]\n",
    "Maximum a posterior (MAP) estimation:\n",
    "\\begin{equation}\n",
    "\\theta_{MAP} = \\underset{\\theta}{\\textrm{argmax}}  \\log P(\\mu, \\delta |D) \\propto \\underset{\\theta}{\\textrm{argmax}}  \\log P(D|\\mu, \\delta)P(\\mu)\n",
    "\\end{equation}\n",
    "According to step 4 to get the corresponding derivatives of $\\mu$ and $\\delta$:\n",
    "TODO\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
